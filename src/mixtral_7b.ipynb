{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook running\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "import ast\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import json\n",
    "from datetime import datetime\n",
    "import requests\n",
    "from datasets import load_dataset\n",
    "import random\n",
    "from transformers import AutoTokenizer\n",
    "print(\"Notebook running\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # login via the huggingface hub with you hf_token\n",
    "# # you need a huggingface account and create a token here: https://huggingface.co/settings/tokens\n",
    "# # we can then call on the token with huggingface_hub.get_token()\n",
    "# import huggingface_hub\n",
    "# huggingface_hub.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /Users/radhikagaonkar/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "!huggingface-cli login --token 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"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global variables for experiment variations\n",
    "API_URL = \"https://api-inference.huggingface.co/models/mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
    "SEED = 42\n",
    "N_SAMPLE = False  # You can sample parts of the data for faster testing. False for run on full dataset, int for sampling\n",
    "SELF_CONSISTENCY_ITERATIONS = 3  # How many times should the model try to predict the same text for self-consistency?\n",
    "DATA_SUBSET = \"sentences_allagree\"  # \"sentences_allagree\", \"sentences_66agree\", \"sentences_75agree\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_str =  json.dumps({\n",
    "#             \"elem_type\": \"NarrativeText\",\n",
    "#             \"elem_content\": \"Section 1. Purpose. Artificial intelligence (AI) holds extraordinary potential for both promise and peril. Responsible AI use has the potential to help solve urgent challenges while making our world more prosperous, productive, innovative, and secure. At the same time, irresponsible use could exacerbate societal harms such as fraud, discrimination, bias, and disinformation; dis- place and disempower workers; stifle competition; and pose risks to national security. Harnessing AI for good and realizing its myriad benefits requires mitigating its substantial risks. This endeavor demands a society-wide effort that includes government, the private sector, academia, and civil society.\"\n",
    "#         })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_message(doc):\n",
    "    user_message = f\"\"\"\n",
    "    Input:\n",
    "    Markdown document:\n",
    "    {doc}\n",
    "\n",
    "    Instructions:\n",
    "    Analyze the provided markdown document and identify words and phrases that would be difficult for laymen or everyday people to understand. This includes:\n",
    "    - Technical jargon: Words specific to a particular field or industry.\n",
    "    - Archaic language: Words or phrases that are no longer commonly used.\n",
    "    - Legal terminology: Words or phrases with specific legal meanings.\n",
    "    - Complex sentence structures: Sentences that are long, convoluted, or difficult to parse.\n",
    "\n",
    "    Classify the identified words and phrases into three categories based on their difficulty level:\n",
    "    - Novice: Words or phrases that are unfamiliar to most people.\n",
    "    - Intermediate: Words or phrases that are somewhat familiar but may require additi\n",
    "    \n",
    "    \n",
    "    \n",
    "    onal context to understand.\n",
    "    - Expert: Words or phrases that are highly technical or specialized and require significant expertise to understand.\n",
    "\n",
    "    For each category, create a dictionary where the key is the word/phrase and the value is the corresponding sentence or phrase where it appears in the document.\n",
    "\n",
    "    Generate a JSON object with the following structure:\n",
    "    {{\n",
    "        \"novice\": {{\n",
    "            \"[Word/Phrase 1]\": \"[Explanation of the Word/Phrase 1 given the text context in which it occurs]\",\n",
    "            ...\n",
    "        }},\n",
    "        \"intermediate\": {{\n",
    "            \"[Word/Phrase 1]\": \"[Explanation of the Word/Phrase 2 given the text context in which it occurs]\",\n",
    "            ...\n",
    "        }},\n",
    "        \"expert\": {{\n",
    "            \"[Word/Phrase 1]\": \"[Explanation of the Word/Phrase 3 given the text context in which it occurs]\",\n",
    "            ...\n",
    "        }}\n",
    "    }}\n",
    "\n",
    "    Example:\n",
    "    Markdown Input:\n",
    "    # Mr. James\n",
    "\n",
    "    Mr. James is a 37-year-old, chronically ill individual with a history of respiratory problems. He has been diagnosed with **pulmonary fibrosis**, a progressive lung disease that causes scarring and stiffness in the lungs. This condition makes it difficult for Mr. James to breathe and perform daily activities.\n",
    "\n",
    "    JSON Output:\n",
    "    {{\n",
    "        \"novice\": {{\n",
    "            \"respiratory problems\": \"Respiratory problems are conditions that affect breathing, such as asthma and bronchitis.\",\n",
    "            \"chronically ill\": \"A person who has a long-lasting illness or health condition that requires continuous medical care and management.\",\n",
    "            \"pulmonary fibrosis\": \"Pulmonary fibrosis is a lung disease characterized by the scarring and thickening of lung tissue, leading to difficulty in breathing and decreased oxygen levels in the blood.\",\n",
    "            \n",
    "        }},\n",
    "        \"intermediate\": {{\n",
    "            \"chronically ill\": \"A person who has a long-lasting illness or health condition that requires continuous medical care and management.\",\n",
    "        }},\n",
    "        \"expert\": {{\n",
    "            \"pulmonary fibrosis\": \"Pulmonary fibrosis is a lung disease characterized by the scarring and thickening of lung tissue, leading to difficulty in breathing and decreased oxygen levels in the blood.\"\n",
    "        }}\n",
    "    }}\n",
    "\n",
    "    Logically, the novice category should contain any phrases that occur in intermediate or expert category. Similarly, the intermediate collection should contain any phrases that occur in expert category.\n",
    "    Your selection and categorization of words should make logical sense.\n",
    "    \n",
    "    Finally, it is highly important to stick to the output format. Here are the expected high-level JSON keys: text, novice, intermediate, expert\n",
    "    \"\"\"\n",
    "\n",
    "    return user_message.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels = [\"positive\", \"negative\", \"neutral\"]\n",
    "\n",
    "# def clean_output(string, random_choice=True):\n",
    "#     for category in labels:\n",
    "#         if category.lower() in string.lower():\n",
    "#             return category\n",
    "#     # if the output string cannot be mapped to one of the categories, we either return \"FAIL\" or choose a random label\n",
    "#     if random_choice:\n",
    "#         return random.choice(labels)\n",
    "#     else:\n",
    "#         return \"FAIL\"\n",
    "\n",
    "\n",
    "# def process_output_cot(output):\n",
    "#     try: \n",
    "#         output_dic = ast.literal_eval(output) \n",
    "#         return output_dic\n",
    "#     except Exception as e:\n",
    "#         # if json/dict parse fails, do simple search for occurance of first label term\n",
    "#         print(f\"Parsing failed for output: {output}, Error: {e}\")\n",
    "#         output_cl = clean_output(output, random_choice=False)\n",
    "#         output_dic = {\"reason\": \"FAIL\", \"label\": output_cl}\n",
    "#         return output_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# docs on different parameters: https://huggingface.co/docs/api-inference/detailed_parameters#text-generation-task\n",
    "generation_params = dict(\n",
    "    top_p=0.90,\n",
    "    temperature=0.8,\n",
    "    max_new_tokens=10000,\n",
    "    return_full_text=False,\n",
    "    use_cache=False,\n",
    ")\n",
    "\n",
    "def generate_text(prompt=None, generation_params=None):\n",
    "    payload = {\n",
    "        \"inputs\": prompt, \n",
    "        \"parameters\": {**generation_params}\n",
    "    }\n",
    "    response = requests.post(\n",
    "                API_URL, \n",
    "                headers={\"Authorization\": f\"Bearer {huggingface_hub.get_token()}\"}, \n",
    "                json=payload\n",
    "        )\n",
    "    return response.json()[0][\"generated_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "def run_mistral(user_message):\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mixtral-8x7B-Instruct-v0.1\")\n",
    "\n",
    "    jargon_phrase_process = [{\"role\": \"user\", \"content\": user_message}]\n",
    "    # jargon_phrase_process_cot = [{\"role\": \"user\", \"content\": prompt_financial_sentiment_cot}]\n",
    "\n",
    "    prompt_jargon_phrase_process = tokenizer.apply_chat_template(jargon_phrase_process, tokenize=False)\n",
    "    # prompt_jargon_phrase_process_cot = tokenizer.apply_chat_template(chat_financial_sentiment_cot, tokenize=False)\n",
    "    output = generate_text(prompt=prompt_jargon_phrase_process, generation_params=generation_params)\n",
    "    return output\n",
    "\n",
    "def run_llama3(user_message):\n",
    "    \n",
    "    model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "    \n",
    "    jargon_phrase_process = [{\"role\": \"system\", \"content\": user_message}]\n",
    "    # jargon_phrase_process_cot = [{\"role\": \"user\", \"content\": prompt_financial_sentiment_cot}]\n",
    "\n",
    "    input_ids = tokenizer.apply_chat_template(jargon_phrase_process, tokenize=False, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    terminators = [\n",
    "        tokenizer.eos_token_id,\n",
    "        tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "    ]\n",
    "    \n",
    "    outputs = model.generate(\n",
    "        input_ids,\n",
    "        max_new_tokens=256,\n",
    "        eos_token_id = terminators,\n",
    "        do_sample=True,\n",
    "        temperature=0.6,\n",
    "        top_p=0.9\n",
    "        \n",
    "    )\n",
    "    \n",
    "    response = outputs[0][input_ids.shape[-1]:]\n",
    "    output = tokenizer.decode(response, skip_special_tokens=True)\n",
    "    # print(output)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_output(output_str):\n",
    "    start = output_str.find(\"`json\") + 6   #Find start of the JSON block and adjust index past '`json'\n",
    "    end = output_str.find(\"```\", start)\n",
    "    json_block = output_str[start:end].strip() #Extract and strip any leading/trailing whitespace\n",
    "    return json_block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import json\n",
    "input_json_files = [\"formatted_data_biosafety.json\"]\n",
    "\n",
    "print(\"Preprocessing the data:\\n\")\n",
    "\n",
    "#Prepare a histogram from the lengths of the different narrative texts\n",
    "text_length = list()\n",
    "\n",
    "for input_file in input_json_files:\n",
    "    # Load the JSON file\n",
    "    with open(input_file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # Initialize the output dictionary\n",
    "    all_elems_json=[]\n",
    "\n",
    "    # Find the \"Text\" element and extract its content\n",
    "    count_elems = 0\n",
    "    error_rows = 0\n",
    "\n",
    "    for element in data['elements']:\n",
    "        \n",
    "        # if count_elems == 50:\n",
    "        #     break\n",
    "\n",
    "        if element['elem_type'] == 'NarrativeText':\n",
    "            json_output = {\n",
    "                'text': \"\",\n",
    "            }\n",
    "            print(\"Size of narrative text:\", len(element['elem_content'].split(\" \")))\n",
    "            text_length.append(len(element['elem_content'].split(\" \")))\n",
    "            \n",
    "            # if len(element['elem_content'].split(\" \")) > 70:\n",
    "            #     print(element['elem_content'])\n",
    "            #     print(\"   \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming text_length is your list containing lengths of different texts\n",
    "plt.hist(text_length, bins='auto')  # 'auto' automatically determines the number of bins\n",
    "plt.title(\"Histogram of Text Lengths\")\n",
    "plt.xlabel(\"Text Length\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import json\n",
    "import huggingface_hub\n",
    "\n",
    "for input_file in input_json_files:\n",
    "    # Load the JSON file\n",
    "    with open(input_file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # Initialize the output dictionary\n",
    "    all_elems_json=[]\n",
    "\n",
    "    # Find the \"Text\" element and extract its content\n",
    "    count_elems = 0\n",
    "    error_rows = 0\n",
    "\n",
    "    for element in data['elements']:\n",
    "        \n",
    "        if count_elems == 10:\n",
    "            break\n",
    "\n",
    "        if element['elem_type'] == 'NarrativeText':\n",
    "            json_output = {\n",
    "                'text': \"\",\n",
    "            }\n",
    "            print(\"Size of narrative text:\", len(element['elem_content'].split(\" \")))\n",
    "            \n",
    "            if len(element['elem_content'].split(\" \")) > 70:\n",
    "                print(element['elem_content'])\n",
    "                print(\"   \")\n",
    "                json_output['text'] = element['elem_content']\n",
    "                \n",
    "                imp_words_output = run_mistral(user_message(json.dumps(json_output)))\n",
    "                print(\"Raw extracted output:\")\n",
    "                print(\"   \")\n",
    "                print(imp_words_output)\n",
    "                print(\"   \")\n",
    "                imp_words_clean = clean_output(imp_words_output)\n",
    "                print(\"Extracted jargon words:\")\n",
    "                print(\"  \")\n",
    "                print(imp_words_clean)\n",
    "                print(\"   \")\n",
    "                try:\n",
    "                    json_output = json.loads(imp_words_clean)\n",
    "                    print(json_output)\n",
    "                    print(\"   \")\n",
    "                    result_url_output = copy.deepcopy(json_output)\n",
    "                    \n",
    "                    for key in json_output:\n",
    "                        if key in [\"novice\", \"intermediate\", \"expert\"]:\n",
    "                            words_dict = json_output[key]\n",
    "                            print(\"List of extracted words for:\", key)\n",
    "                            print(words_dict.keys())\n",
    "                            \n",
    "                            if len(words_dict.keys()) > 0:\n",
    "                                for word in words_dict:\n",
    "                            #         query_str = f\"\"\"What would be the meaning of this word or phrase, '{word}' which occurs in the following sentence '{words_dict[word]}'\"\"\"\n",
    "                            #         print(\"Query:\")\n",
    "                            #         print(query_str)\n",
    "                            #         print(\" \")\n",
    "                            #         try:\n",
    "                            #             search_result = call_brave(query_str)\n",
    "                            #             print(\"Search result:\")\n",
    "                            #             print(search_result)\n",
    "                            #             print(\"Number of search results:\", len(search_result))\n",
    "                            #             print(search_result[0])\n",
    "                            #             print(search_result[0].keys())\n",
    "                            #             print(search_result[0][\"url\"])\n",
    "                            #             print(search_result[0][\"description\"])\n",
    "                            #             result_url_output[key][word] = {\"explanation\": json_output[key][word], \"url\": str(search_result[0][\"url\"]), \"title\": search_result[0][\"title\"]}\n",
    "                            #             print(\"   \")\n",
    "\n",
    "                            #         except Exception as e:\n",
    "                            #             print(e)\n",
    "                            #             result_url_output[key][word] = {\"explanation\": json_output[key][word], \"url\": \"\", \"title\": \"\"}\n",
    "                                    result_url_output[key][word] = {\"text\": element[\"elem_content\"], \"explanation\": json_output[key][word], \"url\": \"\", \"title\": \"\"}\n",
    "                except Exception as e:\n",
    "                    print(\"Error in parsing:\")\n",
    "                    print(e)\n",
    "                    # result_url_output = {\"text\": element['elem_content'], \"novice\": {}, \"intermediate\": {}, \"expert\": {}}\n",
    "                    # error_rows += 1\n",
    "\n",
    "            # else:\n",
    "            #     result_url_output = {\"text\": element['elem_content'], \"novice\": {}, \"intermediate\": {}, \"expert\": {}}\n",
    "            #     error_rows += 1                    \n",
    "\n",
    "                    \n",
    "            print(\"   \")\n",
    "            \n",
    "            print(\"-------------------------------------------------------------------------\")\n",
    "            \n",
    "            print(\"    \")\n",
    "            \n",
    "            all_elems_json.append(result_url_output)\n",
    "            count_elems += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "docu_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
